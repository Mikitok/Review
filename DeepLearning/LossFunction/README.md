## Questions

**Q：介绍一下L1范数与L2范数**

对一个向量来说，L1范数是所有维度的绝对值之和，L2范数是所有维度的值的平方和的开方。

**Q：L1损失函数（MAE）和L2损失函数（MSE）各有什么优缺点？**

1. L1损失函数的优点是对异常值不敏感，鲁棒性更好。缺点是曲线在x=0处不可导，而且大部分情况下梯度都是相等的，这意味着即使对于小的损失值，其梯度也是大的。这不利于函数的收敛和模型的学习。
2. L2损失函数优点是计算简单，处处可导，便于使用梯度下降算法。而且，随着误差的减小，梯度也在减小，这有利于收敛，即使使用固定的学习速率，也能较快的收敛到最小值。缺点是对于异常点比较敏感，因为L2损失函数有平方求和的运算，所以如果样本中存在离群点，MSE会给离群点更高的权重，这就会牺牲其他正常点数据的预测效果，最终降低整体的模型性能。 

**Q：Faster R-CNN的损失函数是什么？相较于MAE和MSE有什么改进？**

1. Faster R-CNN的损失函数是Smooth L1损失，是在MAE上的改进版。在x的绝对值小于1时，SmoothL1值为0.5x的平方，而其他情况下，值为x的绝对值-0.5。
2. MSE当损失增大时，梯度也很大。而MAE当损失很小时，L1的导数的绝对值仍为1，如果learning rate不是自动衰减的话，拿损失函数就会在稳定值附近波动，难以达到更好的效果。而Smooth L1损失在损失很大时，限制梯度的绝对值的上限为1，在损失较小时，梯度也会变小。相比于L1损失函数，可以收敛得更快，相比于L2损失函数，对离群点、异常值不敏感，梯度变化相对更小，训练时不容易跑飞。

**Q：L1正则为什么更容易获得稀疏解？**

引入L2正则时，代价函数在0处的导数无变化，但是引入L1正则后，代价函数在0处的导数有一个突变，则在0处会是一个极小值点。因此，优化时，很可能优化到该极小值点上。

这个茴字写法太多，放一个[大全](https://zhuanlan.zhihu.com/p/50142573)。

**Q：交叉熵为什么有负号？**

负号是为了让Loss为正，这样loss可以越来越小。

**Q：MSE和交叉熵的区别**
1. 通常来说，MSE用于回归问题，交叉熵用于分类问题。
2. 如果将MSE用于分类问题的话，根据计算公式来说，交叉熵在标签为0处是可以不用计算的，而MSE都要算一遍。MSE无差别得关注全部类别上预测概率和真实概率的差，交叉熵关注的是正确类别的预测概率。

## References
1. [l1正则与l2正则的特点是什么，各有什么优势？](https://www.zhihu.com/question/26485586)
2. [回归损失函数1：L1 loss, L2 loss以及Smooth L1 Loss的对比](https://www.cnblogs.com/wangguchangqing/p/12021638.html)
