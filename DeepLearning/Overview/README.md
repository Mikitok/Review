## Questions
**Q: Multi-task learning与Transfer Learning有什么相同和区别？**

迁移学习在源领域（即数据）和任务上学习出一般的规律（泛化知识），然后将这个规律迁移到目标领域和任务上。多任务学习是同时学习多个相关任务，让这些任务在学习过程中共享知识，利用多个任务之间的相关性来改进模型在每个任务的性能和泛化能力。

相同点是都需要用到相关任务，都利用到了从相关任务中学习到泛化知识。

不同点如下：
1. 迁移学习能够适用于目标任务数据不足的情况，而多任务学习不适用。
2. 迁移学习通常分为两阶段：源任务上的学习阶段+目标任务上的迁移学习阶段；而多任务学习部分阶段，同时学习。
3. 迁移学习是单向的知识迁移，希望能够提高模型在目标任务上的性能；而多任务学习是希望你提高所有任务的性能，是双向的知识迁移。

**Q：正负样本数据量不等时，可以怎么办？**
1. 重采样。对数据量少的类别重复采样。
2. 欠采样。减少对数量多的类别的采样。
3. 权值调整。加大数目少的类别的样本在训练时候的权重。

**Q：什么是Batchnorm？为什么要进行Batchnorm？**

1. Batchnorm指的是在batch的维度上对数据进行归一化处理。
2. 因为深度神经网络主要就是为了学习训练数据的分布，并在测试集上达到很好的泛化效果。但是，如们每一个batch输入的数据都具有不同的分布，显然会给网络的训练带来困难。另一方面，数据经过一层层网络计算后，其数据分布也在发生着变化，会给下一层的网络学习带来困难。

**Q：Bagging与Boosting的区别**

1. 样本选择上：Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。
2. 样例权重：Bagging：使用均匀取样，每个样例的权重相等；Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。
3. 预测函数：Bagging：所有预测函数的权重相等。Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。
4. 并行计算：Bagging：各个预测函数可以并行生成。Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。
5. bagging是减少variance，而boosting是减少bias

## References
1. [迁移学习和多任务学习](http://muchong.com/t-5675135-1)
2. [迁移学习和多任务学习有什么本质区别？](https://www.zhihu.com/question/282518309)
3. [Multi-task Learning的三个小知识](https://zhuanlan.zhihu.com/p/56613537)
