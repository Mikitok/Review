## Questions
**Q: Multi-task learning与Transfer Learning有什么相同和区别？**

迁移学习在源领域（即数据）和任务上学习出一般的规律（泛化知识），然后将这个规律迁移到目标领域和任务上。多任务学习是同时学习多个相关任务，让这些任务在学习过程中共享知识，利用多个任务之间的相关性来改进模型在每个任务的性能和泛化能力。

相同点是都需要用到相关任务，都利用到了从相关任务中学习到泛化知识。

不同点如下：
1. 迁移学习能够适用于目标任务数据不足的情况，而多任务学习不适用。
2. 迁移学习通常分为两阶段：源任务上的学习阶段+目标任务上的迁移学习阶段；而多任务学习部分阶段，同时学习。
3. 迁移学习是单向的知识迁移，希望能够提高模型在目标任务上的性能；而多任务学习是希望你提高所有任务的性能，是双向的知识迁移。

**Q：正负样本数据量不等时，可以怎么办？**
1. 重采样。对数据量少的类别重复采样。
2. 欠采样。减少对数量多的类别的采样。
3. 权值调整。加大数目少的类别的样本在训练时候的权重。

**Q：什么是Batchnorm？为什么要进行Batchnorm？**

1. Batchnorm指的是在batch的维度上对数据进行归一化处理。
2. 因为深度神经网络主要就是为了学习训练数据的分布，并在测试集上达到很好的泛化效果。但是，如们每一个batch输入的数据都具有不同的分布，显然会给网络的训练带来困难。另一方面，数据经过一层层网络计算后，其数据分布也在发生着变化，会给下一层的网络学习带来困难。

## References
1. [迁移学习和多任务学习](http://muchong.com/t-5675135-1)
2. [迁移学习和多任务学习有什么本质区别？](https://www.zhihu.com/question/282518309)
3. [Multi-task Learning的三个小知识](https://zhuanlan.zhihu.com/p/56613537)
