## Questions

**Q：在传统的RNN网络中可以用ReLU作为激活函数么？**

可以，但是需要对矩阵的初值进行一定限制，否则或引发梯度消失或者梯度爆炸的问题。因为根据使用了ReLUde激活函数的循环神经网络的梯度计算公式，梯度传了n层之后会有一个系数是W的n次方，如果W不是单位矩阵，就会出现梯度爆炸或者消失的问题。

**Q：为什么sigmoid和Tanh会出现梯度消失的现象**

1. sigmoid是一个S型的曲线，当x很大时，激活函数的值趋近1；当x很小时，激活函数的值趋近0。它的导数是f(x)(1-f(x))，所以当x很大或者很小时导数都会趋近0，从而造成梯度消失的现象。
2. tanh也是一个S型的曲线，当x很大时，激活函数的值趋近1；当x很小时，激活函数的值趋近-1。它的导数是1-f(x)^2，所以当x很大或者很小时导数都会趋近0，从而造成梯度消失的现象。

**Q：Relu为什么可以处理梯度消失的问题？**

relu激活函数为max(0, x)，它的导数当x>0时是1，当x<0时是0，

**Q：Relu相对于sigmoid和Tanh有什么优点？**

1. 从计算上，sigmoid和Tanh有大量的指数运算，而relu只需要一个阈值就可以得到激活值。
2. relu的非饱和性可以解决梯度消失问题。
3. relu的单侧抑制提供了网络稀疏表达的能力。

**Q：Relu相对于sigmoid和Tanh有什么局限性？**

由于relu是单侧激活，所以在训练过程中会有神经元死亡的问题，即流经改神经元的梯度永远为0.所以在实际训练中，如果学习率设置过大，会有超过一定比例的神经元不可逆死亡，进而参数无法更新，导致训练过程失败。
