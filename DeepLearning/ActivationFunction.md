## Questions

**Q：在传统的RNN网络中可以用ReLU作为激活函数么？**

可以，但是需要对矩阵的初值进行一定限制，否则或引发梯度消失或者梯度爆炸的问题。因为根据使用了ReLUde激活函数的循环神经网络的梯度计算公式，梯度传了n层之后会有一个系数是W的n次方，如果W不是单位矩阵，就会出现梯度爆炸或者消失的问题。
