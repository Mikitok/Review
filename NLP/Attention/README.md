## Questions
**Q: 软注意力机制和硬注意力机制有什么区别？**

1. 硬注意力机制选择某个部分作为注意力部分，该部分注意力权重为1，其他部分的注意力权重为0。
2. 软注意力机制中每个部分的注意力权重在0～1之间，且求和为1。

**Q: 相较于硬注意力机制，软注意力机制有什么优点？**

硬性注意力的一个缺点是基于最大采样或随机采样的方式来选择信息。因此最终的损失函数与注意力分布之间的函数关系不可导，因此无法使用在反向传播算法进行训练。软注意力是一个完全可微的确定性过程，梯度能够在通过注意力机制传播的同时向网络其他部分传播。

**Q: 不使用注意力机制的卷积或循环神经网络在处理长序列时有什么缺点？**

无论卷积还是循环神经网络其实都是对变长序列的一种“局部编码”：卷积神经网络显然是基于N-gram的局部编码；而对于循环神经网络，由于梯度消失等问题也只能建立短距离依赖。

**Q: 如果想进一步在卷积或循环神经网络的基础上输入序列建立长距离依赖关系，有哪些办法？**

1. 增加网络的层数，通过一个深层网络来获取远距离的信息交互。
2. 全连接层，但是无法处理变长的输入序列。
3. 注意力机制。

**Q: multi-head的实现方式**

将一个词的词向量切分成h个块，求attention相似度时是一个句子中每个词之间第i个块的相似度。原论文是有h次线性映射，后来的bert是切分为h个部分。

**Q: multi-head的意义**

由于单词映射在高维空间作为向量形式，每一维空间都可以学到不同的特征，相邻空间所学结果更相似，相较于全体空间放到一起对应更加合理。比如对于vector-size=512的词向量，取h=8，每64个空间做一个attention，学到结果更细化。


## References
1. [Attention机制详解（一）](https://zhuanlan.zhihu.com/p/47063917)
2. [Attention机制详解（二）](https://zhuanlan.zhihu.com/p/47282410)
3. [各种注意力机制](https://blog.csdn.net/qq_32806793/article/details/88782157)
4. [Attention 和self-attention](https://www.cnblogs.com/AntonioSu/p/12019534.html)


