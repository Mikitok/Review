## Question 

**Q：决策树的内部结点和叶子结点有什么区别？**

决策树的内部节点表示一个特征或者属性，而叶子节点通常表示一个类。

**Q：在构建决策树的过程中，每一条路径要满足互斥且完备的要求，请问怎么理解互斥和完备？**

1. 完备指的是构建过程中每一个实例都应该被一条路径或者规则覆盖。
2. 互斥指的是构建过程中每一个实例仅被一条路径或者规则覆盖。

**Q：请简单描述一下决策树的构建过程**

1. 决策树的构建过程是一个递归的过程。他首先选取最优的特征（标准可以是信息增益、信息增益比等），然后根据特征对训练数据进行分割，使得对各个子数据集有一个最好的分类。然后递归的进行。
2. 如果有需要的话，可以对决策树进一步进行剪枝。

**Q：什么是熵？熵值的大小表示什么？**

熵是表示随机变量不确定性的度量。一般来说，熵值越大，不确定性就越大。

**Q：信息熵怎么求？**

对每一类求-p log p，然后进行求和。

**Q：什么是信息增益？**

信息增益是知道特征X的信息而使得类Y的信息不确定性减少的程度。

**Q：什么是信息增益比？**

特征A对数据集D的信息增益比定义为其信息增益与训练数据集D关于特征值A的值的熵之比。

**Q：选取信息增益比作为特征选择的评估方法比信息增益有什么优点？**

根据信息增益的计算公式，以信息增益作为划分训练数据集的特征，会倾向于选择取值较多的特征。而信息增益比将信息增益除以数据集关于这个特征的熵，可以对信息增益进行约束。

**Q：ID3算法和C4.5作为两种常用的决策树算法，有什么区别？**

1. ID3算法的核心是在决策树各个结点的特征选择使用信息增益作为标准。
2. C4.5算法的核心是在决策树各个结点的特征选择使用信息增益比作为标准。

**Q：ID3和C4.5分裂后，结点的信息熵是变大还是变小？**

结点的信息熵减小，因为有了进一步的划分之后不确定性降低了，熵也会降低。

**Q：ID3有什么缺点？**

1. ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。
2. ID3采用信息增益大的特征优先建立决策树的节点。但是在相同条件下，会倾向于选择取值比较多的特征。
3. ID3算法对于缺失值的情况没有做考虑。
4. 没有考虑过拟合的问题。

**Q：C4.5有什么缺点？**

1. 没有考虑过拟合的问题。C4.5的剪枝方法有优化的空间。思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。主要采用的是后剪枝加上交叉验证选择最合适的决策树。
2. C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。
3. C4.5只能用于分类，不可用于回归。
4. C4.5由于使用了熵模型，里面有大量的耗时的对数运算和排序运算。

**Q：决策树在遇到连续特征如何建树？**

C4.5和CART算法中都有针对连续特征的处理。具体的思路如下，比如m个样本的连续特征A有m个，从小到大排列为a1,a2,...,ama1,a2,...,am,则CART算法取相邻两样本值的中位数，一共取得m-1个划分点。C4.5然后以信息增益比来计算，而CART以平方误差（回归）或者基尼系数（分类）来计算。

**Q：决策树的剪枝时候如何实现的？**

将原来损失函数加上一个控制模型复杂度的参数项，然后通过最小化损失函数来剪枝。具体来说，如果决策树过于复杂，那么预测误差会减小，但是相对来说后面控制复杂度的一项会增大，所以就是在这两项之间求得一个平衡。

具体实现上是由下至上实现的，就是对一个点计算剪枝之前和剪枝之后对应的损失值，如果剪了更下就剪掉，知道不能继续为止，则得到损失函数最小的子树。
