## Question 

**Q：介绍一下决策树**

决策树是一种基本分类方法。决策树的内部节点表示一个特征或者属性，而叶子节点通常表示一个类。可以将决策树看成一种if-then的结构，从根节点到叶子节点一条路径就是一个决策规则。学习时，利用训练数据，根据训练损失最小化的原则简历决策树模型，预测时，用决策树进行分类。常用的决策树有ID3，C4.5，CART等。

**Q：决策树的内部结点和叶子结点有什么区别？**

决策树的内部节点表示一个特征或者属性，而叶子节点通常表示一个类。

**Q：在构建决策树的过程中，每一条路径要满足互斥且完备的要求，请问怎么理解互斥和完备？**

1. 完备指的是构建过程中每一个实例都应该被一条路径或者规则覆盖。
2. 互斥指的是构建过程中每一个实例仅被一条路径或者规则覆盖。

**Q：请简单描述一下决策树的构建过程**

1. 决策树的构建过程是一个递归的过程。他首先选取最优的特征（标准可以是信息增益、信息增益比等），然后根据特征对训练数据进行分割，使得对各个子数据集有一个最好的分类。然后递归的进行。
2. 如果有需要的话，可以对决策树进一步进行剪枝。

**Q：什么是熵？熵值的大小表示什么？**

熵是表示随机变量不确定性的度量。一般来说，熵值越大，不确定性就越大。

**Q：信息熵怎么求？**

对每一类求-p log p，然后进行求和。

**Q：什么是信息增益？**

信息增益是知道特征X的信息而使得类Y的信息不确定性减少的程度。

**Q：什么是信息增益比？**

特征A对数据集D的信息增益比定义为其信息增益与训练数据集D关于特征值A的值的熵之比。

**Q：选取信息增益比作为特征选择的评估方法比信息增益有什么优点？**

根据信息增益的计算公式，以信息增益作为划分训练数据集的特征，会倾向于选择取值较多的特征。而信息增益比将信息增益除以数据集关于这个特征的熵，可以对信息增益进行约束。

**Q：ID3算法和C4.5作为两种常用的决策树算法，有什么区别？**

1. ID3算法的核心是在决策树各个结点的特征选择使用信息增益作为标准。
2. C4.5算法的核心是在决策树各个结点的特征选择使用信息增益比作为标准。

**Q：ID3和C4.5分裂后，结点的信息熵是变大还是变小？**

结点的信息熵减小，因为有了进一步的划分之后不确定性降低了，熵也会降低。

**Q：ID3有什么缺点？**

1. ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。
2. ID3采用信息增益大的特征优先建立决策树的节点。但是在相同条件下，会倾向于选择取值比较多的特征。
3. ID3算法对于缺失值的情况没有做考虑。
4. 没有考虑过拟合的问题。

**Q：C4.5有什么缺点？**

1. 没有考虑过拟合的问题。C4.5的剪枝方法有优化的空间。思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。主要采用的是后剪枝加上交叉验证选择最合适的决策树。
2. C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。
3. C4.5只能用于分类，不可用于回归。
4. C4.5由于使用了熵模型，里面有大量的耗时的对数运算和排序运算。

**Q：介绍一下CART方法**

CART中文名是分类与回归树，是决策树的一种，是C4.5的改进版。首先它既可以用于分类，也可以用于回归。分类时使用基尼系数（选最小的）作为特征选择的方法，回归时使用平方误差（选最小的）作为特征选择的方法。另外与C4.5不同，CART是一个二叉树，即对于表示属性的内部节点只有两个子结点。

**Q：基尼系数怎么算？**

对于每个类求p^2并求和得到v，然后1-v。

**Q：决策树在遇到连续特征如何建树？**

C4.5和CART算法中都有针对连续特征的处理。具体的思路如下，比如m个样本的连续特征A有m个，从小到大排列为a1,a2,...,ama1,a2,...,am,则CART算法取相邻两样本值的中位数，一共取得m-1个划分点。C4.5然后以信息增益比来计算，而CART以平方误差（回归）或者基尼系数（分类）来计算。

**Q：决策树的剪枝时候如何实现的？**

将原来损失函数加上一个控制模型复杂度的参数项，然后通过最小化损失函数来剪枝。具体来说，如果决策树过于复杂，那么预测误差会减小，但是相对来说后面控制复杂度的一项会增大，所以就是在这两项之间求得一个平衡。

具体实现分为预剪枝和后剪枝。
1. 预剪枝：对每个结点划分前先进行估计，若当前结点的划分不能带来决策树的泛化性能的提升，则停止划分，并标记为叶结点。
2. 后剪枝：现从训练集生成一棵完整的决策树，然后自底向上对非叶子结点进行考察，若该结点对应的子树用叶结点能带来决策树泛化性能的提升，则将该子树替换为叶结点。

**Q：归一化对决策树有什么影响？**

没有影响。决策树关注的是变量的分布和变量之间的条件概率，取值范围对他没有什么影响。

**Q：决策树参数**

1.criterion gini(基尼系数) or entropy(信息熵)  
2.splitter best or random 前者是在所有特征中找最好的切分点 后者是在部分特征中（数据量大的时候）
3.max_features None（所有），log2，sqrt，N 特征小于50的时候一般使用所有的
4.max_depth 数据少或者特征少的时候可以不管这个值，如果模型样本量多，特征也多的情况下，可以尝试限制下
5.min_samples_split 如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。
6.min_samples_leaf 这个值限制了叶子节点最少的样本数，如果某叶子节点样本数目小于min_samples_leaf，则会和兄弟节点一起被剪枝，如果样本量不大，不需要管这个值，大些如10W可是尝试下5
7.min_weight_fraction_leaf 这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。
8.max_leaf_nodes 通过限制最大叶子节点数，可以防止过拟合，默认是"None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制具体的值可以通过交叉验证得到。
9.class_weight 指定样本各类别的的权重，主要是为了防止训练集某些类别的样本过多导致训练的决策树过于偏向这些类别。这里可以自己指定各个样本的权重如果使用“balanced”，则算法会自己计算权重，样本量少的类别所对应的样本权重会高。
10.min_impurity_split 这个值限制了决策树的增长，如果某节点的不纯度(基尼系数，信息增益，均方差，绝对差)小于这个阈值则该节点不再生成子节点。即为叶子节点 。


## References
1. [剪枝处理](https://www.cnblogs.com/lsm-boke/p/12256686.html)
2. [决策树参数介绍](https://www.cnblogs.com/mdevelopment/p/9381726.html)
