## Questions

**Q：对k近邻算法来说最重要的三个因素是什么？**

1. k值的选择。k值过小的话预测结果会对邻近点非常的敏感，意味着整体模型变得复杂，容易发生过拟合。k值过大的话，模型整体变得简单，但是这时候较远的点也会对预测起作用。所以一般是选择一个比较小的数值，然后通过交叉验证选择最合适的。
2. 距离度量方法。距离度量方法反映了两个点的相似程度，常用的有曼哈顿距离、欧式距离等。
3. 分类决策规则。k近邻中的分类决策规则往往是多数表决法（经验风险最小化）。

**Q：k近邻算法k的选择对结果的影响？**

1. 如果选择较小的K值，就相当于用较小的邻域中的训练实例进行预测，“学习”的近似误差会减小，只有输入实例较近的训练实例才会对预测结果起作用。但缺点是“学习”的估计误差会增大，预测结果会对近邻实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错。换句话说，k值得减小就意味着整体模型非常复杂，容易发生过拟合。 
2. 如果选择较大的k值，就相当于用较大邻域中的训练实例进行预测，其实有点是减少学习的估计误差，但缺点是学习的近似误差会增大。这时与输入实例较远的训练实例也会起预测作用，使预测发生错误，k值得增大就意味着整体的模型变得简单。
3. 在应用中，k值一般取一个比较小的数值，通常采用交叉验证法来选择最优k值。

**Q：什么是kd树？**

Kd-树是K-dimension tree的缩写，是对数据点在k维空间中划分的一种数据结构，主要应用于多维空间关键数据的搜索。本质上说，Kd-树就是一种平衡二叉树。

**Q：kd树用在KNN的哪个步骤里？**

kd树是用在k近邻的搜索过程中。因为如果对每个样本都算一次的话计算量太大，所以就有kd树，可以省去对大部分点的搜索。

**Q：kd树的作用？**

kd树的作用是减少搜索过程中的计算量。因为直接一一匹配计算的话，计算的复杂度在O(n)。但是kd树是在训练数据集的基础上构建得到的一个二叉平衡树，将一一匹配的计算转为一个对树的搜索，可以省去对大部分点的计算，从而可以将复杂度降到O(logn)。

**Q：kd树是怎么构建的？**

(1) 开始：选择第一维为坐标轴，以这一维的中位数作为切分，将所有的点分成两个子区域（一边大一边小）。
(2) 重复：对于深度为j的结点，选择第l维(l = j%k+1)作为坐标轴，并以这一维的中位数作为切分，将所有的点分成两个子区域（一边大一边小）。直到两个子区域没有点的时候停止。

## References
1. [kd树踩坑指南](https://www.nowcoder.com/discuss/327012?type=post&order=time&pos=&page=1&channel=666&source_id=search_post)
